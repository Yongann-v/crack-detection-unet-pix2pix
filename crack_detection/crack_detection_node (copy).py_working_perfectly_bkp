#!/usr/bin/env python3

import os
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import String, Bool, Int32
from geometry_msgs.msg import Point
from cv_bridge import CvBridge
import cv2
import numpy as np
import torch
import gc
from collections import deque
import time
from datetime import datetime

import albumentations as A
from albumentations.pytorch import ToTensorV2
from ament_index_python.packages import get_package_share_directory

# Import your models
from crack_detection.unet_model import UNet
from crack_detection.train_pix2pix import Generator
from crack_detection.unet_rt_inference_tiling6 import TiledUNetInference
from crack_detection.pix2pix_rt_inference_tiling import TiledRefinedInference


class CrackDetectionNode(Node):
    """ROS2 node for crack detection with UNet and Pix2Pix support."""
    
    def __init__(self):
        super().__init__('crack_detection_node')
        
        # Declare ROS2 parameters
        self.declare_parameters(
            namespace='',
            parameters=[
                ('model_path', 'models/best_model.pth'),
                ('pix2pix_model_path', 'models/pix2pix_epoch_98_best.pth'),
                ('input_size', 384),
                ('threshold', 0.5),
                ('min_crack_percent', 5.0),
                ('use_tiling', False),
                ('use_pix2pix', False),
                ('window_size', 384),
                ('subdivisions', 2),
                ('skip_frames', 1),
                ('camera_topic', '/camera/color/image_raw'),
                ('publish_visualization', True),
                # New zoom parameters
                ('zoom_enabled', False),
                ('zoom_factor', 2.0),
                ('zoom_center_x', 0.5),
                ('zoom_center_y', 0.5),
                # Image capture parameters
                ('save_directory', 'crack_captures'),
                ('capture_key_topic', '/capture_image'),
            ]
        )
        
        # Get parameters
        model_path_param = self.get_parameter('model_path').value
        pix2pix_path_param = self.get_parameter('pix2pix_model_path').value
        
        # Convert relative paths to absolute using package share directory
        if not os.path.isabs(model_path_param):
            package_share = get_package_share_directory('crack_detection')
            self.model_path = os.path.join(package_share, model_path_param)
        else:
            self.model_path = model_path_param
            
        if not os.path.isabs(pix2pix_path_param):
            package_share = get_package_share_directory('crack_detection')
            self.pix2pix_model_path = os.path.join(package_share, pix2pix_path_param)
        else:
            self.pix2pix_model_path = pix2pix_path_param
        
        self.input_size = self.get_parameter('input_size').value
        self.threshold = self.get_parameter('threshold').value
        self.min_crack_percent = self.get_parameter('min_crack_percent').value
        self.use_tiling = self.get_parameter('use_tiling').value
        self.use_pix2pix = self.get_parameter('use_pix2pix').value
        self.window_size = self.get_parameter('window_size').value
        self.subdivisions = self.get_parameter('subdivisions').value
        self.skip_frames = self.get_parameter('skip_frames').value
        self.camera_topic = self.get_parameter('camera_topic').value
        self.publish_visualization = self.get_parameter('publish_visualization').value
        
        # Get zoom parameters
        self.zoom_enabled = self.get_parameter('zoom_enabled').value
        self.zoom_factor = self.get_parameter('zoom_factor').value
        self.zoom_center_x = self.get_parameter('zoom_center_x').value
        self.zoom_center_y = self.get_parameter('zoom_center_y').value
        
        # Get capture parameters
        self.save_directory = self.get_parameter('save_directory').value
        self.capture_key_topic = self.get_parameter('capture_key_topic').value
        
        # Create save directory if it doesn't exist
        if not os.path.isabs(self.save_directory):
            self.save_directory = os.path.join(os.getcwd(), self.save_directory)
        os.makedirs(self.save_directory, exist_ok=True)
        
        self.get_logger().info('='*60)
        self.get_logger().info('Crack Detection Node Initializing...')
        self.get_logger().info('='*60)
        self.get_logger().info(f'UNet model path: {self.model_path}')
        if self.use_pix2pix:
            self.get_logger().info(f'Pix2Pix model path: {self.pix2pix_model_path}')
        self.get_logger().info(f'Detection threshold: {self.threshold}')
        self.get_logger().info(f'Minimum crack %: {self.min_crack_percent}%')
        self.get_logger().info(f'Frame skipping: Process every {self.skip_frames} frame(s)')
        self.get_logger().info(f'Camera topic: {self.camera_topic}')
        
        # Log zoom settings
        if self.zoom_enabled:
            self.get_logger().info(f'Zoom enabled: {self.zoom_factor}x at ({self.zoom_center_x:.2f}, {self.zoom_center_y:.2f})')
        else:
            self.get_logger().info('Zoom disabled')
        
        self.get_logger().info(f'Save directory: {self.save_directory}')
        
        # Setup device
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.get_logger().info(f'Device: {self.device}')
        
        # Determine mode and initialize appropriate models
        self.determine_and_initialize_mode()
        
        # Initialize CV Bridge
        self.bridge = CvBridge()
        
        # Frame counting and caching
        self.frame_count = 0
        self.last_pred_prob = None
        self.last_binary_mask = None
        self.last_crack_percentage = 0.0
        self.last_original_frame = None  # Store original frame for saving
        
        # Zoom region cache
        self.zoom_roi = None
        
        # FPS tracking
        self.fps_queue = deque(maxlen=30)
        self.last_time = time.time()
        
        # Create subscribers
        self.image_sub = self.create_subscription(
            Image,
            self.camera_topic,
            self.image_callback,
            10
        )
        
        # Subscribe to capture trigger (can be triggered by keyboard or button)
        self.capture_sub = self.create_subscription(
            Bool,
            self.capture_key_topic,
            self.capture_callback,
            10
        )
        
        # Create publishers
        self.detection_pub = self.create_publisher(
            String,
            '/crack_detection/result',
            10
        )
        
        self.crack_detected_pub = self.create_publisher(
            Bool,
            '/crack_detection/detected',
            10
        )
        
        self.crack_center_pub = self.create_publisher(
            Point,
            '/crack_detection/center_pixel',
            10
        )
        
        if self.publish_visualization:
            self.viz_pub = self.create_publisher(
                Image,
                '/crack_detection/visualization',
                10
            )
        
        self.get_logger().info('âœ“ Initialization complete!')
        self.get_logger().info('Waiting for camera images...')
        self.get_logger().info('='*60)
    
    def calculate_zoom_roi(self, frame_shape):
        """Calculate the Region of Interest for zooming."""
        height, width = frame_shape[:2]
        
        # Calculate new dimensions after zoom
        new_width = int(width / self.zoom_factor)
        new_height = int(height / self.zoom_factor)
        
        # Calculate center point in pixels
        center_x_px = int(width * self.zoom_center_x)
        center_y_px = int(height * self.zoom_center_y)
        
        # Calculate ROI boundaries
        x1 = max(0, center_x_px - new_width // 2)
        y1 = max(0, center_y_px - new_height // 2)
        x2 = min(width, x1 + new_width)
        y2 = min(height, y1 + new_height)
        
        # Adjust if ROI goes out of bounds
        if x2 - x1 < new_width:
            if x1 == 0:
                x2 = min(width, new_width)
            else:
                x1 = max(0, width - new_width)
        
        if y2 - y1 < new_height:
            if y1 == 0:
                y2 = min(height, new_height)
            else:
                y1 = max(0, height - new_height)
        
        return (x1, y1, x2, y2)
    
    def apply_zoom(self, frame):
        """Apply zoom to the frame."""
        if not self.zoom_enabled or self.zoom_factor <= 1.0:
            return frame, None
        
        # Calculate ROI if not cached or frame size changed
        if self.zoom_roi is None or self.zoom_roi[2] - self.zoom_roi[0] != int(frame.shape[1] / self.zoom_factor):
            self.zoom_roi = self.calculate_zoom_roi(frame.shape)
        
        x1, y1, x2, y2 = self.zoom_roi
        
        # Extract ROI
        zoomed_frame = frame[y1:y2, x1:x2]
        
        # Resize back to original dimensions
        zoomed_frame = cv2.resize(zoomed_frame, (frame.shape[1], frame.shape[0]), 
                                 interpolation=cv2.INTER_LINEAR)
        
        return zoomed_frame, self.zoom_roi
    
    def map_coordinates_to_original(self, x, y, zoom_roi):
        """Map coordinates from zoomed image back to original image."""
        if zoom_roi is None:
            return x, y
        
        x1, y1, x2, y2 = zoom_roi
        roi_width = x2 - x1
        roi_height = y2 - y1
        
        # Map from zoomed (resized) coordinates to original
        original_x = int(x * roi_width / self.last_original_frame.shape[1] + x1)
        original_y = int(y * roi_height / self.last_original_frame.shape[0] + y1)
        
        return original_x, original_y
    
    def capture_callback(self, msg):
        """Callback for image capture trigger."""
        if msg.data and self.last_binary_mask is not None and self.last_original_frame is not None:
            # Generate timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
            
            # Save original frame
            original_path = os.path.join(self.save_directory, f"frame_{timestamp}.png")
            cv2.imwrite(original_path, self.last_original_frame)
            
            # Save binary mask
            mask_path = os.path.join(self.save_directory, f"mask_{timestamp}.png")
            cv2.imwrite(mask_path, self.last_binary_mask)
            
            # Save probability map as heatmap
            if self.last_pred_prob is not None:
                heatmap = (self.last_pred_prob * 255).astype(np.uint8)
                heatmap_colored = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
                heatmap_path = os.path.join(self.save_directory, f"heatmap_{timestamp}.png")
                cv2.imwrite(heatmap_path, heatmap_colored)
            
            # Save info text file
            info_path = os.path.join(self.save_directory, f"info_{timestamp}.txt")
            with open(info_path, 'w') as f:
                f.write(f"Frame: {self.frame_count}\n")
                f.write(f"Crack Percentage: {self.last_crack_percentage:.2f}%\n")
                f.write(f"Zoom Enabled: {self.zoom_enabled}\n")
                f.write(f"Zoom Factor: {self.zoom_factor}\n")
                f.write(f"Mode: {self.mode}\n")
                f.write(f"Threshold: {self.threshold}\n")
            
            self.get_logger().info(f'ðŸ“¸ Captured images saved to {self.save_directory} with timestamp {timestamp}')
    
    def determine_and_initialize_mode(self):
        """Determine which inference mode to use and initialize models."""
        
        if self.use_tiling and self.use_pix2pix:
            # Mode 4: UNet + Pix2Pix Tiled (Best Quality)
            self.mode = "UNET_PIX2PIX_TILED"
            self.get_logger().info(f'Mode: UNet + Pix2Pix TILED (Best Quality)')
            self.get_logger().info(f'Window size: {self.window_size}x{self.window_size}')
            self.get_logger().info(f'Subdivisions: {self.subdivisions}')
            
            self.tiled_refined_inferencer = TiledRefinedInference(
                unet_path=self.model_path,
                pix2pix_path=self.pix2pix_model_path,
                device=self.device,
                window_size=self.window_size,
                subdivisions=self.subdivisions
            )
            self.model = None
            self.pix2pix = None
            self.transforms = None
            self.tiled_inferencer = None
            
        elif self.use_tiling and not self.use_pix2pix:
            # Mode 2: UNet Tiled Only
            self.mode = "UNET_TILED"
            self.get_logger().info(f'Mode: UNet TILED (High Accuracy)')
            self.get_logger().info(f'Window size: {self.window_size}x{self.window_size}')
            self.get_logger().info(f'Subdivisions: {self.subdivisions}')
            
            self.tiled_inferencer = TiledUNetInference(
                model_path=self.model_path,
                device=self.device,
                window_size=self.window_size,
                subdivisions=self.subdivisions
            )
            self.model = None
            self.pix2pix = None
            self.transforms = None
            self.tiled_refined_inferencer = None
            
        elif not self.use_tiling and self.use_pix2pix:
            # Mode 3: UNet + Pix2Pix Fast
            self.mode = "UNET_PIX2PIX_FAST"
            self.get_logger().info(f'Mode: UNet + Pix2Pix FAST (Better Quality)')
            self.get_logger().info(f'Input size: {self.input_size}x{self.input_size}')
            
            self.model, self.pix2pix = self.load_both_models()
            self.transforms = self.get_inference_transforms()
            self.tiled_inferencer = None
            self.tiled_refined_inferencer = None
            
        else:
            # Mode 1: UNet Fast Only (Default)
            self.mode = "UNET_FAST"
            self.get_logger().info(f'Mode: UNet FAST (Real-time)')
            self.get_logger().info(f'Input size: {self.input_size}x{self.input_size}')
            
            self.model = self.load_unet_model()
            self.pix2pix = None
            self.transforms = self.get_inference_transforms()
            self.tiled_inferencer = None
            self.tiled_refined_inferencer = None
    
    def load_unet_model(self):
        """Load UNet model only."""
        self.get_logger().info(f'Loading UNet from: {self.model_path}')
        
        model = UNet(
            num_classes=1,
            align_corners=False,
            use_deconv=False,
            in_channels=3
        ).to(self.device)
        
        checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        self.get_logger().info('âœ“ UNet loaded successfully!')
        return model
    
    def load_both_models(self):
        """Load both UNet and Pix2Pix models."""
        self.get_logger().info(f'Loading UNet from: {self.model_path}')
        
        # Load UNet
        unet = UNet(
            num_classes=1,
            align_corners=False,
            use_deconv=False,
            in_channels=3
        ).to(self.device)
        
        unet_checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)
        unet.load_state_dict(unet_checkpoint['model_state_dict'])
        unet.eval()
        
        self.get_logger().info(f'Loading Pix2Pix from: {self.pix2pix_model_path}')
        
        # Load Pix2Pix
        pix2pix = Generator().to(self.device)
        
        pix2pix_checkpoint = torch.load(self.pix2pix_model_path, map_location=self.device, weights_only=False)
        pix2pix.load_state_dict(pix2pix_checkpoint['generator_state_dict'])
        pix2pix.eval()
        
        self.get_logger().info('âœ“ Both models loaded successfully!')
        return unet, pix2pix
    
    def get_inference_transforms(self):
        """Get transforms for inference."""
        return A.Compose([
            A.Resize(self.input_size, self.input_size),
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            ),
            ToTensorV2()
        ])
    
    def predict_frame_unet_fast(self, frame_rgb):
        """Mode 1: Fast UNet prediction."""
        original_size = frame_rgb.shape[:2]
        
        # Apply transforms
        transformed = self.transforms(image=frame_rgb)
        input_tensor = transformed['image'].unsqueeze(0).to(self.device)
        
        # Predict
        with torch.no_grad():
            output = self.model(input_tensor)
            if isinstance(output, (tuple, list)):
                output = output[0]
            
            prediction = torch.sigmoid(output)
            
            # Resize back to original size
            pred_resized = torch.nn.functional.interpolate(
                prediction,
                size=original_size,
                mode='bilinear',
                align_corners=False
            )
            
            # Convert to numpy
            pred_numpy = pred_resized.squeeze().cpu().numpy()
            
            # Apply threshold
            binary_mask = (pred_numpy > self.threshold).astype(np.uint8) * 255
        
        # Calculate crack percentage
        crack_pixels = np.sum(binary_mask > 127)
        total_pixels = binary_mask.size
        crack_percentage = (crack_pixels / total_pixels) * 100
        
        return pred_numpy, binary_mask, crack_percentage
    
    def predict_frame_unet_tiled(self, frame_rgb):
        """Mode 2: Tiled UNet prediction."""
        original_size = frame_rgb.shape[:2]
        
        # Pad image
        padded = self.tiled_inferencer._pad_img(frame_rgb)
        padded_shape = list(padded.shape[:-1]) + [1]
        
        # Process with tiling
        subdivs = self.tiled_inferencer._windowed_subdivs(padded)
        predictions = self.tiled_inferencer._recreate_from_subdivs(subdivs, padded_shape)
        
        # Unpad
        predictions = self.tiled_inferencer._unpad_img(predictions)
        
        # Crop to original size
        pred_numpy = predictions[:original_size[0], :original_size[1], 0]
        
        # Apply threshold
        binary_mask = (pred_numpy > self.threshold).astype(np.uint8) * 255
        
        # Calculate crack percentage
        crack_pixels = np.sum(binary_mask > 127)
        total_pixels = binary_mask.size
        crack_percentage = (crack_pixels / total_pixels) * 100
        
        # Cleanup
        gc.collect()
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()
        
        return pred_numpy, binary_mask, crack_percentage
    
    def predict_frame_pix2pix_fast(self, frame_rgb):
        """Mode 3: Fast UNet + Pix2Pix prediction."""
        original_size = frame_rgb.shape[:2]
        
        # Apply transforms
        transformed = self.transforms(image=frame_rgb)
        input_tensor = transformed['image'].unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            # Stage 1: UNet prediction
            unet_output = self.model(input_tensor)
            if isinstance(unet_output, (tuple, list)):
                unet_output = unet_output[0]
            unet_pred = torch.sigmoid(unet_output)
            
            # Stage 2: Pix2Pix residual refinement
            residual = self.pix2pix(unet_pred)
            
            # Stage 3: Combine predictions
            refined_pred = torch.clamp(unet_pred + residual, 0, 1)
            
            # Resize back to original size
            pred_resized = torch.nn.functional.interpolate(
                refined_pred,
                size=original_size,
                mode='bilinear',
                align_corners=False
            )
            
            # Convert to numpy
            pred_numpy = pred_resized.squeeze().cpu().numpy()
            
            # Apply threshold
            binary_mask = (pred_numpy > self.threshold).astype(np.uint8) * 255
        
        # Calculate crack percentage
        crack_pixels = np.sum(binary_mask > 127)
        total_pixels = binary_mask.size
        crack_percentage = (crack_pixels / total_pixels) * 100
        
        return pred_numpy, binary_mask, crack_percentage
    
    def predict_frame_pix2pix_tiled(self, frame_rgb):
        """Mode 4: Tiled UNet + Pix2Pix prediction."""
        original_size = frame_rgb.shape[:2]
        
        # Pad image
        padded = self.tiled_refined_inferencer._pad_img(frame_rgb)
        padded_shape = list(padded.shape[:-1]) + [1]
        
        # Process with tiling + Pix2Pix
        subdivs = self.tiled_refined_inferencer._windowed_subdivs(padded)
        predictions = self.tiled_refined_inferencer._recreate_from_subdivs(subdivs, padded_shape)
        
        # Unpad
        predictions = self.tiled_refined_inferencer._unpad_img(predictions)
        
        # Crop to original size
        pred_numpy = predictions[:original_size[0], :original_size[1], 0]
        
        # Apply threshold
        binary_mask = (pred_numpy > self.threshold).astype(np.uint8) * 255
        
        # Calculate crack percentage
        crack_pixels = np.sum(binary_mask > 127)
        total_pixels = binary_mask.size
        crack_percentage = (crack_pixels / total_pixels) * 100
        
        # Cleanup
        gc.collect()
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()
        
        return pred_numpy, binary_mask, crack_percentage
    
    def predict_frame(self, frame_rgb):
        """Route to appropriate prediction method based on mode."""
        if self.mode == "UNET_FAST":
            return self.predict_frame_unet_fast(frame_rgb)
        elif self.mode == "UNET_TILED":
            return self.predict_frame_unet_tiled(frame_rgb)
        elif self.mode == "UNET_PIX2PIX_FAST":
            return self.predict_frame_pix2pix_fast(frame_rgb)
        elif self.mode == "UNET_PIX2PIX_TILED":
            return self.predict_frame_pix2pix_tiled(frame_rgb)
    
    def calculate_crack_center(self, binary_mask):
        """Calculate the center point of detected cracks."""
        crack_pixels = np.where(binary_mask > 127)
        
        if len(crack_pixels[0]) > 0:
            center_y = int(np.mean(crack_pixels[0]))
            center_x = int(np.mean(crack_pixels[1]))
            return center_x, center_y
        else:
            return 0, 0
    
    def create_visualization(self, frame_bgr, pred_prob, binary_mask, crack_percentage, 
                           is_crack_detected, zoom_roi):
        """Create visualization with heatmap and overlay."""
        # Store original frame for zoom visualization
        original_frame = self.last_original_frame.copy() if self.last_original_frame is not None else frame_bgr.copy()
        
        # Draw zoom region on original frame if zoom is enabled
        if self.zoom_enabled and zoom_roi is not None:
            x1, y1, x2, y2 = zoom_roi
            cv2.rectangle(original_frame, (x1, y1), (x2, y2), (0, 255, 255), 2)
            cv2.putText(original_frame, f"Zoom: {self.zoom_factor:.1f}x", 
                       (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        
        # Create heatmap
        heatmap = (pred_prob * 255).astype(np.uint8)
        heatmap_colored = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
        
        # Create crack overlay
        overlay = frame_bgr.copy()
        crack_pixels = binary_mask > 127
        overlay[crack_pixels] = [0, 0, 255]  # Red for cracks
        blended = cv2.addWeighted(frame_bgr, 0.7, overlay, 0.3, 0)
        
        # Stack horizontally (show original with zoom box, heatmap, and blended)
        combined = np.hstack([original_frame, heatmap_colored, blended])
        
        # Add info panel
        panel_height = 80
        info_panel = np.zeros((panel_height, combined.shape[1], 3), dtype=np.uint8)
        
        # FPS
        fps = self.calculate_fps()
        cv2.putText(info_panel, f"FPS: {fps:.1f}", (10, 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # Crack percentage
        cv2.putText(info_panel, f"Crack: {crack_percentage:.2f}%", (10, 40),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # Zoom status
        zoom_text = f"Zoom: {self.zoom_factor:.1f}x" if self.zoom_enabled else "Zoom: OFF"
        cv2.putText(info_panel, zoom_text, (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # Mode
        cv2.putText(info_panel, f"Mode: {self.mode}", (250, 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
        
        # Frame count
        cv2.putText(info_panel, f"Frame: {self.frame_count}", (250, 40),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
        
        # Capture instruction
        cv2.putText(info_panel, "Publish Bool to /capture_image to save", (250, 60),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (150, 150, 255), 1)
        
        # Detection status
        if is_crack_detected:
            status_text = "CRACK DETECTED"
            status_color = (0, 0, 255)  # Red
        else:
            status_text = "NO CRACK"
            status_color = (0, 255, 0)  # Green
        
        text_x = combined.shape[1] - 240
        cv2.putText(info_panel, status_text, (text_x, 40),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, status_color, 2)
        
        # Combine with info panel
        result = np.vstack([info_panel, combined])
        
        return result
    
    def calculate_fps(self):
        """Calculate current FPS."""
        current_time = time.time()
        time_diff = current_time - self.last_time
        self.last_time = current_time
        
        if time_diff > 0:
            fps = 1.0 / time_diff
            self.fps_queue.append(fps)
        
        return np.mean(self.fps_queue) if len(self.fps_queue) > 0 else 0.0
    
    def image_callback(self, msg):
        """Callback function for incoming camera images."""
        self.frame_count += 1
        
        # Convert ROS Image to OpenCV
        try:
            frame_bgr = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
        except Exception as e:
            self.get_logger().error(f'Failed to convert image: {e}')
            return
        
        # Store original frame for saving and visualization
        self.last_original_frame = frame_bgr.copy()
        
        # Apply zoom if enabled
        zoomed_frame_bgr, zoom_roi = self.apply_zoom(frame_bgr)
        
        # Convert BGR to RGB for model (use zoomed frame)
        frame_rgb = cv2.cvtColor(zoomed_frame_bgr, cv2.COLOR_BGR2RGB)
        
        # Check if we should process this frame
        if self.frame_count % self.skip_frames == 1 or self.skip_frames == 1:
            # Run inference on zoomed frame
            pred_prob, binary_mask, crack_percentage = self.predict_frame(frame_rgb)
            
            # Cache results
            self.last_pred_prob = pred_prob
            self.last_binary_mask = binary_mask
            self.last_crack_percentage = crack_percentage
        else:
            # Use cached results
            pred_prob = self.last_pred_prob
            binary_mask = self.last_binary_mask
            crack_percentage = self.last_crack_percentage
        
        # Determine if crack detected
        is_crack_detected = crack_percentage >= self.min_crack_percent
        
        # Calculate crack center (in zoomed frame coordinates)
        center_x_zoomed, center_y_zoomed = self.calculate_crack_center(binary_mask)
        
        # Map coordinates back to original frame if zoom is enabled
        if self.zoom_enabled and zoom_roi is not None:
            center_x, center_y = self.map_coordinates_to_original(
                center_x_zoomed, center_y_zoomed, zoom_roi
            )
        else:
            center_x, center_y = center_x_zoomed, center_y_zoomed
        
        # Print detection result
        if is_crack_detected:
            zoom_info = f" [Zoom: {self.zoom_factor}x]" if self.zoom_enabled else ""
            self.get_logger().info(
                f'ðŸ”´ CRACK DETECTED at pixel ({center_x}, {center_y}){zoom_info} | '
                f'Coverage: {crack_percentage:.2f}% | Frame: {self.frame_count}'
            )
        
        # Publish detection result
        result_msg = String()
        result_msg.data = (f'detected={is_crack_detected},crack_percent={crack_percentage:.2f},'
                          f'center_x={center_x},center_y={center_y},frame={self.frame_count},'
                          f'zoom_enabled={self.zoom_enabled},zoom_factor={self.zoom_factor}')
        self.detection_pub.publish(result_msg)
        
        # Publish boolean detection
        detected_msg = Bool()
        detected_msg.data = bool(is_crack_detected)
        self.crack_detected_pub.publish(detected_msg)
        
        # Publish crack center (in original frame coordinates)
        if is_crack_detected:
            center_msg = Point()
            center_msg.x = float(center_x)
            center_msg.y = float(center_y)
            center_msg.z = 0.0
            self.crack_center_pub.publish(center_msg)
        
        # Publish visualization
        if self.publish_visualization:
            viz_image = self.create_visualization(
                zoomed_frame_bgr, pred_prob, binary_mask, crack_percentage, 
                is_crack_detected, zoom_roi
            )
            
            try:
                viz_msg = self.bridge.cv2_to_imgmsg(viz_image, encoding='bgr8')
                viz_msg.header = msg.header
                self.viz_pub.publish(viz_msg)
            except Exception as e:
                self.get_logger().error(f'Failed to publish visualization: {e}')


def main(args=None):
    rclpy.init(args=args)
    
    node = CrackDetectionNode()
    
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info('Shutting down crack detection node...')
    finally:
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
